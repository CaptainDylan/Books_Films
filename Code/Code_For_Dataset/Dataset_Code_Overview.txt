
To run our project, you only need the Read It or Watch It.qvf file in the main CODE folder. 

Project instructions also ask for code used to get and process our dataset. Qlik is directly connected to a mysql database where we have stored our dataset after gathering, processing, and topic analysis. Below is an overview of the steps and code files used for these steps and algorithms that happened OUTSIDE of the Qlik .qvf file to store, update, or add data in mysql that feeds into the .qvf file. Code files can be found in their numbered subfolders. 


1. Getting Book/Movie Pairs
--------------------------
* Run script to scrape Wikipedia pages of book/movie pairs: Scrape_Wiki_Films.ipynb 
* Run script to scrape ISBNs of books from linked Wikipedia pages for as many books as possible in the list: Get Books ISBNs.ipynb


2. Book Data Collection
--------------------------
* Signed up for goodreads API key: https://www.goodreads.com/api
* Took Book/Movie pair list from Wikipedia scrape and do a little bit of manual editing on book titles on series that didn't scrape cleanly from 
* Get Goodreads Book IDs: Get_Book_Ids_Mapping_file.ipynb
    * Use data from Book/Movie pair list (isbn, title, author, alternative title) and iterate through the best choices for finding a match to get goodreads id using python script
* Take output of this file (a csv) -include the title/author for the match from goodreads and manually check titles that didn't have an exact match on isbn or title+author and make manual adjustments on goodreads_id column when the match is bad. All done in Excel.
* Input this cleaned up file into scraping script: goodreads_data_collection.py 
    * This script web scrapes Goodreads for data about the book, book genres, and the first 300 reviews for each book (if >300). Note for our scraping, we used a rotating proxy service to ensure the IP would not get blocked but have removed that personal info from the script so if you run this script, should be aware of that risk. The script will take several days to run when used on the full list.
* Send the data to mysql: send_bookdata_to_mysql.ipynb


3. Movie Data Collection
--------------------------
* Movie TMDb Data API data collection 
    * Pulling the movie database data.ipynb - in this script, we pull TMDb id and release dates. The release date is used to help match mvoie IDs to movie names in the Wikipedia list.  This script also corrected some troublesome items (for instance, Harry Potter was listed under one release date in the Wiki list) and drops items that could not be matched.  Finally, it links up the TMBd details with the Goodreads details.
    * Getting imdb ids.ipynb - the next step was to obtain the IMDB id.  This script simply pulls the matching IMDb id for each TMDb id in our dataset.
    * Collect budget revenue runtime and posterpath.ipynb - As the title of this script mentions, this script collects additional information from IMDb.  
* Download Movie IMDB datasets here: https://www.imdb.com/interfaces/
    * movie-basics.tsv, Map imdb to genre.ipynb - this file and script were used to obtain genre information
    * imdb-cast.tsv, Adding actor director data.ipynb - this file and script were used to obtain cast information
    * imdb-ratings.tsv, Get rating and number of votes from IMDB.ipynb - this file and script were used to get movie ratings and # of votes
* Once you have the imdbids from TMDb API, run IMdb review scraping script: ScrapeIMDB.py
    * Upload output to mySQL

4. Data Processing 
--------------------------
* Run scripts to remove non-English reviews: Update Review Language.ipynb, Language Filter.ipynb
* Connecting book/movie data: 
    * This was done within the Pulling the movie database data.ipynb file.  
* Run script for genre cleaning/standardization: genre_standardization.ipynb
* Run script for rating standardization: overallrating_standardize.ipynb
* Review Text Cleaning scripts: 
    * book_review_cleaning2.ipynb and movie_review_cleaning2.ipynb
	* These were used to clean the review text and remove stop words, standardize misspelled words, and lemmatize
* book_review_cleaning_TopA_xT.ipynb, movie_review_cleaning_TopA_xT.ipynb	
	* These are used to clean the text more for the word clouds, includes removing the title of the book/movie to help declutter the cloud.


5. Text Analysis
--------------------------
*  LDA_with_gensim.ipynb
    *  Used to create several LDA topic models with varying number of topics; Runs coherence on each model to track the best model; Run separately for book reviews and movie reviews
*  Get LDA Stats.ipynb
	*  Get some statistics on the generated topics to compare
*  Assign_Book_Review_Topics.ipynb, Assign_Movie_Review_Topics.ipynb
	*  Take the preferred topic model and assign topics to individual reviews
*  Assign_Book_Topics.ipynb, Assign_Movie_Topics.ipynb
	*  Take the preferred topic model and assign topics to books/movies by combining all the reviews into one “document”
*  Visualization.ipynb, Visualization-Books.ipynb
	*  Use LDAVis to get an interactive visualization of the topics
*  Other code in R was used to investigate ideas in sentiment analysis prior to working through the LDA process.  This code was not used to create our final dataset and so has not been included here.
