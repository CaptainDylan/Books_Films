{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in each topic model.  Get the topic/word matrix.  Analyze the probabilities by word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic models were created in the \"LDA with gensim\" notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load('movies_dictionary.gensim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'movies_topics_5.gensim'\n",
    "ldatmp = LdaModel.load(fname) #, mmap='r')\n",
    "topic_matrix = ldatmp.get_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_matrix.shape\n",
    "# (5, 310703)\n",
    "# Sum up probabilities for each word.  I don't know if this is strictly correct, but it seems to be useful\n",
    "words = topic_matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0.09066486\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "print(words.argmax())\n",
    "print(words[words.argmax()])\n",
    "print(dictionary.id2token[words.argmax()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"the\" shouldn't appear in the dictionary.  This told me there was something wrong with my stop word processing.  I should have used lower() before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(words, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would 0.013930077\n",
      "even 0.013995692\n",
      "first 0.0142959645\n",
      "much 0.014359055\n",
      "make 0.014730598\n",
      "book 0.016440477\n",
      "scene 0.016828872\n",
      "also 0.017105663\n",
      "great 0.01711501\n",
      "this 0.017664328\n",
      "well 0.018167106\n",
      "good 0.020014334\n",
      "like 0.022106498\n",
      "time 0.02227082\n",
      "story 0.023099009\n",
      "character 0.023885917\n",
      "one 0.036454484\n",
      "movie 0.05619169\n",
      "film 0.080125116\n",
      "the 0.09066486\n"
     ]
    }
   ],
   "source": [
    "for i in indices[-20:]:\n",
    "    print (dictionary.id2token[i], words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic size:  5\n",
      "would 0.013930077\n",
      "even 0.013995692\n",
      "first 0.0142959645\n",
      "much 0.014359055\n",
      "make 0.014730598\n",
      "book 0.016440477\n",
      "scene 0.016828872\n",
      "also 0.017105663\n",
      "great 0.01711501\n",
      "this 0.017664328\n",
      "well 0.018167106\n",
      "good 0.020014334\n",
      "like 0.022106498\n",
      "time 0.02227082\n",
      "story 0.023099009\n",
      "character 0.023885917\n",
      "one 0.036454484\n",
      "movie 0.05619169\n",
      "film 0.080125116\n",
      "the 0.09066486\n",
      "topic size:  10\n",
      "best 0.027888313\n",
      "make 0.028402815\n",
      "get 0.028670745\n",
      "scene 0.030265296\n",
      "see 0.03033463\n",
      "bond 0.03127066\n",
      "well 0.03158938\n",
      "great 0.033602793\n",
      "also 0.03361028\n",
      "love 0.033639748\n",
      "this 0.03673403\n",
      "good 0.0401398\n",
      "character 0.041948467\n",
      "time 0.042088628\n",
      "story 0.04238884\n",
      "like 0.044474747\n",
      "one 0.07356834\n",
      "movie 0.123443045\n",
      "film 0.13525663\n",
      "the 0.1612034\n",
      "topic size:  15\n",
      "scene 0.037786927\n",
      "action 0.038541652\n",
      "novel 0.038737312\n",
      "good 0.0387716\n",
      "tom 0.039147764\n",
      "this 0.03944949\n",
      "also 0.04031747\n",
      "read 0.04056485\n",
      "world 0.04118835\n",
      "war 0.043827083\n",
      "time 0.046212796\n",
      "character 0.047678333\n",
      "bond 0.047996424\n",
      "like 0.04805067\n",
      "story 0.04855518\n",
      "one 0.07973199\n",
      "movie 0.111965194\n",
      "film 0.12534784\n",
      "book 0.13982618\n",
      "the 0.20110098\n",
      "topic size:  20\n",
      "war 0.04880841\n",
      "scene 0.049014173\n",
      "action 0.04914378\n",
      "also 0.049696635\n",
      "best 0.05216153\n",
      "this 0.052516695\n",
      "read 0.05299199\n",
      "good 0.054479707\n",
      "hitchcock 0.055409867\n",
      "time 0.06263398\n",
      "like 0.06385653\n",
      "story 0.06570263\n",
      "character 0.06667417\n",
      "bond 0.081941836\n",
      "one 0.11282583\n",
      "spielberg 0.13885704\n",
      "book 0.17148355\n",
      "movie 0.1745225\n",
      "film 0.17595561\n",
      "the 0.28228995\n",
      "topic size:  25\n",
      "also 0.057717722\n",
      "tom 0.059625402\n",
      "love 0.06076776\n",
      "scene 0.061380256\n",
      "cruise 0.06217681\n",
      "this 0.06386256\n",
      "world 0.06896386\n",
      "like 0.069689035\n",
      "character 0.07017603\n",
      "time 0.07174975\n",
      "story 0.08147082\n",
      "bond 0.08555167\n",
      "vampire 0.091295905\n",
      "war 0.10615684\n",
      "one 0.117334165\n",
      "spielberg 0.1296892\n",
      "book 0.13635464\n",
      "film 0.19157815\n",
      "movie 0.23122565\n",
      "the 0.31505874\n",
      "topic size:  30\n",
      "child 0.0588883\n",
      "also 0.06397564\n",
      "moore 0.06492824\n",
      "world 0.06635943\n",
      "this 0.06670902\n",
      "bond 0.067494534\n",
      "vampire 0.07178584\n",
      "like 0.07180634\n",
      "love 0.07625196\n",
      "character 0.07651346\n",
      "time 0.07653941\n",
      "war 0.07783687\n",
      "story 0.08514077\n",
      "book 0.09950308\n",
      "spielberg 0.10442936\n",
      "game 0.12019797\n",
      "one 0.12714049\n",
      "movie 0.19659446\n",
      "film 0.24998444\n",
      "the 0.37195426\n",
      "topic size:  40\n",
      "first 0.085268416\n",
      "this 0.0911394\n",
      "bond 0.09303138\n",
      "like 0.098133795\n",
      "hitchcock 0.09827867\n",
      "time 0.10162999\n",
      "character 0.10366578\n",
      "story 0.10454954\n",
      "cruise 0.105555885\n",
      "edward 0.10690613\n",
      "bella 0.111237906\n",
      "tom 0.116200924\n",
      "book 0.12297882\n",
      "vampire 0.15275422\n",
      "one 0.16727346\n",
      "original 0.1791775\n",
      "version 0.195286\n",
      "film 0.26409596\n",
      "movie 0.3202385\n",
      "the 0.4156179\n",
      "topic size:  50\n",
      "paul 0.09150684\n",
      "world 0.09428183\n",
      "cruise 0.09768538\n",
      "story 0.1014328\n",
      "like 0.10183677\n",
      "character 0.10335583\n",
      "time 0.1072642\n",
      "war 0.1080743\n",
      "horse 0.112061456\n",
      "vampire 0.1141027\n",
      "best 0.119012475\n",
      "jack 0.121141665\n",
      "book 0.12720096\n",
      "edward 0.12962463\n",
      "tom 0.16139339\n",
      "one 0.16534087\n",
      "spielberg 0.17436862\n",
      "film 0.24635544\n",
      "movie 0.29107678\n",
      "the 0.4531207\n",
      "topic size:  60\n",
      "character 0.105324864\n",
      "story 0.10785057\n",
      "douglas 0.11038167\n",
      "cruise 0.11139014\n",
      "jack 0.11427137\n",
      "like 0.11547735\n",
      "time 0.11848081\n",
      "peter 0.1247746\n",
      "game 0.12884688\n",
      "action 0.12975995\n",
      "bella 0.13949253\n",
      "edward 0.1422915\n",
      "spielberg 0.14837527\n",
      "book 0.17155175\n",
      "tom 0.17623107\n",
      "vampire 0.18660851\n",
      "one 0.19491273\n",
      "movie 0.25353757\n",
      "film 0.32549912\n",
      "the 0.4680972\n"
     ]
    }
   ],
   "source": [
    "# These will only exist if you executed the LDA notebook.  I'm not currently checking these in since they're exploratory\n",
    "TOPIC_SIZES = [5, 10, 15, 20, 25, 30, 40, 50, 60]\n",
    "results_cv = []\n",
    "word_dict = defaultdict(int)\n",
    "\n",
    "for topic_size in TOPIC_SIZES:\n",
    "    print ('topic size: ', topic_size)\n",
    "    fname = 'movies_topics_' + str(topic_size) + '.gensim'\n",
    "    ldatmp = LdaModel.load(fname) #, mmap='r')\n",
    "    topic_matrix = ldatmp.get_topics()\n",
    "    words = topic_matrix.sum(axis=0) # sum up each word's probability\n",
    "    indices = np.argsort(words, axis=0)\n",
    "    for i in indices[-20:]:\n",
    "        print (dictionary.id2token[i], words[i])\n",
    "        word_dict[dictionary.id2token[i]] = word_dict[dictionary.id2token[i]] + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This shows how many time each word was in the top 20 for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'would': 1,\n",
       "             'even': 1,\n",
       "             'first': 2,\n",
       "             'much': 1,\n",
       "             'make': 2,\n",
       "             'book': 8,\n",
       "             'scene': 5,\n",
       "             'also': 6,\n",
       "             'great': 2,\n",
       "             'this': 7,\n",
       "             'well': 2,\n",
       "             'good': 4,\n",
       "             'like': 9,\n",
       "             'time': 9,\n",
       "             'story': 9,\n",
       "             'character': 9,\n",
       "             'one': 9,\n",
       "             'movie': 9,\n",
       "             'film': 9,\n",
       "             'the': 9,\n",
       "             'best': 3,\n",
       "             'get': 1,\n",
       "             'see': 1,\n",
       "             'bond': 6,\n",
       "             'love': 3,\n",
       "             'action': 3,\n",
       "             'novel': 1,\n",
       "             'tom': 5,\n",
       "             'read': 2,\n",
       "             'world': 4,\n",
       "             'war': 5,\n",
       "             'hitchcock': 2,\n",
       "             'spielberg': 5,\n",
       "             'cruise': 4,\n",
       "             'vampire': 5,\n",
       "             'child': 1,\n",
       "             'moore': 1,\n",
       "             'game': 2,\n",
       "             'edward': 3,\n",
       "             'bella': 2,\n",
       "             'original': 1,\n",
       "             'version': 1,\n",
       "             'paul': 1,\n",
       "             'horse': 1,\n",
       "             'jack': 2,\n",
       "             'douglas': 1,\n",
       "             'peter': 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is separate analysis to count the word frequencies in the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"..\\\\Data_Processing\\\\cleaned.csv\",encoding=\"utf-8\")\n",
    "texts = reviews[\"cleaned\"].apply(lambda s: s.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusdict = defaultdict(int)\n",
    "for text in texts:\n",
    "    textdict = {}\n",
    "    for word in text:\n",
    "        if word not in textdict:\n",
    "            textdict[word] = 1\n",
    "    for word in textdict:\n",
    "        corpusdict[word] = corpusdict[word] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "totaltexts = len(texts) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263535.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totaltexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one 0.5819682395127782\n",
      "movie 0.708812871155634\n",
      "film 0.6298935625249018\n",
      "the 0.7475629423036788\n"
     ]
    }
   ],
   "source": [
    "for k,v in corpusdict.items():\n",
    "    if v/totaltexts > .5: # and v/totaltexts < .6:\n",
    "        print (k, v/totaltexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
